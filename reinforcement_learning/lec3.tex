\subsection*{Repetition}
Here follow some important concepts that were covered in previous lectures:

\begin{wbox}{}
\begin{itemize}
	\item \textbf{States, actions and rewards}, $s \in \mathcal{S}, a \in \mathcal{A}, r \in \mathcal{R}$
	\item \textbf{Dynamic/model:} $p(s^{\prime},r|s,a)$
	\item \textbf{Policy:} $\pi(a|s)$ (For deterministic policy also $a = \pi(s)$)
	\item \textbf{The return:} 
		\begin{equation}
			G_t = R_{t+1}+\gamma R_{t+2} + \gamma^{2}R_{t+3} + \ldots
		\end{equation}
	\item \textbf{State-value function:} Expected return when starting in s and following policy $\pi$: 
		\begin{equation}
			v_\pi(s) = \mathbb{E}[G_t | S_t =s]
		\end{equation}
	\item \textbf{Action-value function:} Expected return when starting in s, taking action a and \textbf{then} follow policy $\pi$ 
		\begin{equation}
			q_\pi(s,a) = \mathbb{E}[G_t | S_t = s, A_t = a]
		\end{equation}
	\item \textbf{Relations:} 
		\begin{equation}
		\begin{aligned}
			v_\pi = \sum_{a}^{} \pi(a|s) q_\pi(s|a) \\
			q_\pi = \sum_{r, s^{\prime}}^{} p(s^{\prime},r|s,a)[r + \gamma v_\pi(s^{\prime})] 
		\end{aligned}
		\end{equation}
		for a deterministic policy a $a = \pi(s)$ we have $v_\pi(s) = q_\pi(s, \pi(s))$
	\item \textbf{Bellman equation for state-values:}
		\begin{equation}
		\begin{aligned}
			v_\pi(s) = \sum_{a}^{} \pi (a|s) \sum_{r, s^{\prime}}^{} p(s^{\prime},r|s,a)[r + \gamma v_\pi(s^{\prime})] \\
			= \mathbb{E}_\pi [R_{t+1}+\gamma v_\pi(S_{t+1})|S_t = s] 
		\end{aligned}
		\end{equation}
	\item \textbf{Optimal value function:}
		\begin{equation}
		 \begin{aligned}
		 	v_* = \underset{\pi}{\text{max }} v_\pi(s), \text{ for all } s \in \mathcal{S} \\
		 	q_* (s,a) = \underset{\pi}{\text{max }}q_\pi(s,a), \text{ for all } s \in \mathcal{S} \text{ and } a \in \mathcal{A}
		 \end{aligned}
		 \end{equation} 

	\item \textbf{Bellman optimality equation:}
	 \begin{equation}
	 v_*(s) = \underset{a}{\text{max }}q_*(s,a) = \underset{a}{\text{max }}\sum_{r, s^{\prime}}^{}p(s^{\prime},r|s,a)[r+\gamma v_*(s^{\prime})] 
	 \end{equation}
\end{itemize}
\end{wbox}

\section{Dynamic programming}
Dynamic programming is a class of algorithms that solves a problem by breaking it down to smaller, overlapping sub-problems. Solving these sub-problems and combine them to one solution. Methods like value and policy iteration are useful for solving Markov Decision Processes by iteratively updating value functions or policies to find the optimal strategy. For this section we will assume that we know the dynamics $p(s^{\prime},r|s,a)$. 

\begin{itemize}
	\item \textbf{Prediction} 
	\begin{itemize}
		\item Given a policy $\pi$, predict the expected future return from each state.
		\item That is, find the state-value function $v_\pi(s)$
	\end{itemize}
	\item \textbf{Control:}
	\begin{itemize}
		\item Given a MDP, find an optimal policy $\pi_*$
		\item If we first compute $v_*$, then we can use:
			\begin{equation}
			\begin{aligned}
				q_*(s,a) = \sum_{r, s^{\prime}}^{}p(s^{\prime},r|s,a)[r + \gamma v_*(s^{\prime})] \\
				\pi_* = \underset{a}{\arg \text{max } } q_*(s,a)
			\end{aligned}
			\end{equation}
	\end{itemize}
\end{itemize}


\subsection*{Policy evaluation}
For the problem we are given: $\pi$, then we compute $v_\pi(s)$ for all $s \in \mathcal{S}$. Using the bellman equation: 

	\begin{equation}
		v_\pi(s) = \sum_{a}^{} \pi (a|s) \sum_{r,s^{\prime}}^{} p(s^{\prime},r|s,a)[r + \gamma v_\pi(s^{\prime})] 
	\end{equation}
This gives us a system of linear equation that can be solved analytically or with iterative process. For large state and or action space, it's more effective to use the iterative alternative. 

\subsection*{Iterative policy evaluation}
We start of with making some sort of initial guess. $v_0$, for example: $v_0(s) = 0$ for all $s$. In each of the iterations we use the RHS of the bellman equation: 

	\begin{equation}
		v_{k+1}(s) = \sum_{a}^{} \pi (a|s) \sum_{r,s^{\prime}}^{} p(s^{\prime},r|s,a)[r + \gamma v_k(s^{\prime})], \text{ for all } s \in \mathcal{S}
	\end{equation}

If we get to a point were $v_k(s) = v_{k+1}(s)$ then we have reached a $v_k$ that solves the Bellman equation. For convergence it can be shown that $v_k(s)\rightarrow v_\pi(s)  $ as $k \rightarrow \infty$.  This is done using the method of contraction mapping: Let $u_k$ and $v_k$ be two different estimates, then:

	\begin{equation}
		||y_{k+1} - v_{k+1}||_\infty \le \gamma ||u_k - v_k||_\infty
	\end{equation}
with $u_k$ = $v_\pi$

	\begin{equation}
		||v_\pi - v_{k+1}||_\infty \le ||v_\pi - v_k||_\infty
	\end{equation}

\textbf{Bootstrapping} is the process of using the old estimate $v_k$ to improve our new estimate 


\subsection*{Implementation}
For a finite state space $\mathcal{S}$ we can represent the state value function $v(s)$ as an array with one element for each state in $\mathcal{S}$. $v_k \rightarrow v_\pi \text{ as } k \rightarrow \infty$,  but in practice we stop when the difference between the new and old step is small enough. The algorithm is then: 

\begin{wbox}{The algorithm}
\textbf{Synchronous updates:}
 \begin{enumerate}
 	\item Initialize $v_{\text{old}}$ (e.g. $v_{\text{old}} = 0$ for all s)
 	\item For all $s \in \mathcal{S}$:
	 	\begin{equation}
	 		v_{new}(s) = \sum_{a}^{} \pi (a|s) \sum_{r, s^{\prime}}^{} p(s^{\prime},r|s,a)[r + \gamma v_{old} (s^{\prime})]
	 	\end{equation}
	 \item if $|v_{old}(s) - v_{new}(s)| < \text{tol}$ for all s, output $v_{new} $ and stop.  
	 \item Otherwise let : $v_{old} \leftarrow v_{new}$ and go back to step 2. 
	 \item  Asynchronous updates also converge to $v_\pi(s)$ as long as we keep updating all states.
 \end{enumerate}
\textbf{Asynchronous updates: } (in-place updates)
\begin{enumerate}
	\item Start with initial v(s) (e.g. v(s) = 0)
	\item For all $s \in \mathcal{S}$
		\begin{equation}
			v(s) \leftarrow \sum_{a}^{}\pi(a|s) \sum_{r, s^{\prime}}^{} p(s^{\prime},r|s,a)[r+\gamma v(s^{\prime})]
		\end{equation}
	\item If changes in v are small enough we are done, otherwise back to step 2. 
\end{enumerate}
This is easier to implement and only need one array v(s). Notice that now the updates depends on what order we sweep through the states. It also converges to $v_\pi (s)$, often faster. 
\end{wbox}

\subsection*{Policy improvement}
Given a policy $\pi$ we now need to see how to evaluate $v_\pi(s)$, is it possible to find a better policy? That is the policy $\pi^{\prime}$ such that:

	\begin{equation}
		v_{\pi^{\prime}}(s) \ge v_\pi(s), \text{ for all } s \in \mathcal{S}
	\end{equation}

The value of taking the action a in state s ant then following the policy $\pi$ afterwards is given by:

	\begin{equation}
		q_\pi(s,a) = \sum_{r, s^{\prime}}^{} p(s^{\prime},r|s,a)[r + \gamma v_\pi(s^{\prime})]
	\end{equation}
The \textbf{greedily} action with the respect to the action values i.e.:

	\begin{equation}
		\pi^{\prime} (s) = \underset{a}{\arg \text{max }} q_\pi(s,a)
	\end{equation}
	
\subsection*{The policy improvement theorem}
Lets consider the deterministic policy $a = \pi(s)$ (the result holds for stochastic $\pi (a|s)$ also). Then the greedy policy with respect to $v_\pi(s)$ is:

	\begin{equation}
		q_\pi(s, \pi^{\prime} (s)) \ge v_\pi(s) = \underset{a}{\text{max }} q_\pi(s,a)
	\end{equation}
and hence

	\begin{equation}
		q_\pi(s, \pi^{\prime}(s)) = \underset{a}{\text{max }}q_\pi(s,a) \ge q_\pi(s,\pi(s)) = v_\pi(s)
	\end{equation}

\begin{wbox}{The policy improvement Theorem}
If $q_\pi(s,\pi^{\prime}(s)) \ge v_\pi(s)$ fir all $s \in \mathcal{S}$, then

	\begin{equation}
		v_{\pi^{\prime}}(s) \ge v_\pi(s), \text{ for all } s \in \mathcal{S}
	\end{equation} 
\end{wbox}


This means that $\pi^{\prime}(s) = \underset{a}{\arg \text{max }} q_\pi(s,a)$ is as good as, or better than $\pi(s)$


\subsection*{Is this an improvement?}
The policy improvement is given by

	\begin{equation}
		\pi^{\prime} = \underset{a}{\arg \text{max }} q_\pi(s,a) = \underset{a}{\arg \text{max }}\sum_{r, s^{\prime}}^{} p(s^{\prime},r|s,a)[r + \gamma v_\pi(s^{\prime})]
	\end{equation}

this will be \textbf{at least} as good as $\pi$ in all states. But what if there is no improvement? What if $v_\pi(s) = v_{\pi^{\prime}}$ for all s? then $q_\pi(s,a) = q_{\pi^{\prime}}(s,a)$ and:

	\begin{equation}
		v_\pi(s) = v_{\pi^{\prime}} = q_{\pi^{\prime}}(s,a) = \underset{a}{\text{max }} \sum_{r, s^{\prime}}^{} p(s^{\prime},r|s,a) [r + \gamma v_\pi(s^{\prime})] \text{ for all }s
	\end{equation}

This is the Bellman optminality equation, so $\pi$ and $\pi^{\prime}$ are optimal policies! So the \textbf{conclusion} is that $\pi^{\prime}$ will be strictly better than $\pi$, unless $\pi$ is already optimal. 


\subsection*{Policy iteration}
If we start with an initial policy $\pi$:

	\begin{enumerate}
	 	\item \textbf{Polict evaluation (E):} Compute $v_\pi(s)$ for all $s$. The iterative policy evaluation
	 	\item \textbf{Policy improvement (I):} Let $\pi^{\prime}(s) = \underset{a}{\arg \text{max }} q_\pi(s,a)$ for all $s$
	 	\item If we have a improvement go to 1. Otherwise we have found the optimal policy $\pi$  
	 \end{enumerate} 

	\begin{equation}
		\pi_0 \overset{E}{\rightarrow}v_{\pi_0} \overset{I}{\rightarrow} \pi_1 \overset{E}{\rightarrow} v_{\pi_2} \overset{I}{\rightarrow} \pi_2 \ldots \overset{I}{\rightarrow} pi_* \overset{E}{\rightarrow} v_*
	\end{equation}

In the case of a finite MDP this will converge within a finite number of iterations. Some details regarding the implementation: In E: we can start from the previous policy to speed up the computations and in I: if there are several a that maximize $q_\pi(s,a)$, choose one arbitrary or a stochastic policy that picks between them with uniform probability. 

\subsection*{Value iteration}
In policy iteration we do the evaluation complete before we improve the policy, this is called \textbf{generalized policy iteration}. We can also stop the evaluation after just one sweep over all states, this is called \textbf{value iteration}

If we take a look on what happens if we do one iteration of evaluation before we improve, that is for all s:

	\begin{equation}
	\begin{aligned}
		q_{k+1}(s,a) = \sum_{r, s^{\prime}}^{}p(s^{\prime},r|s,a)[r + \gamma v_k(s^{\prime})] \\
		\pi_{k+1}(s) = \underset{a}{\arg \text{max }} q_{k+1}(s,a) \\
		v_{k+1}(s) = q_{k+1}(s, pi_{k+1}(s))
	\end{aligned}
	\end{equation}

Or in one equation:

	\begin{equation}
		v_{k+1}(s) = \sum_{r, s^{\prime}}^{}p(s^{\prime},r|s,a)[r + \gamma v_k(s^{\prime})]
	\end{equation}
With this iteration we will converge to the optimal $v_*(s)$. We can for example use in place updates instead of synchronous updates. 


\subsection*{Value iteration and the Bellman optimality equation}

\begin{itemize}
	\item \textbf{Value iteration:} 
		\begin{equation}
			v_{k+1}(s) = \underset{a}{\text{max }}\sum_{r, s^{\prime}}^{}p(s^{\prime},r|s,a)[r + \gamma v_k(s^{\prime})]
		\end{equation}
	\item \textbf{Fixed point: } if $v_k(s) = v_{k+1}(s)$ for all $s$, then
		\begin{equation}
			v_{k}(s) = \underset{a}{\text{max }}\sum_{r, s^{\prime}}^{}p(s^{\prime},r|s,a)[r + \gamma v_k(s^{\prime})]
		\end{equation}
		This is the Bellman optimality function, so $v_k(s)$ is the optimal value function
	\item \textbf{Optimal policy: } When converged to $v_*$ we can find an optimal policy;
		\begin{equation}
			\pi_*(s) = \underset{a}{\arg \text{max }} g_*(s,a)
		\end{equation}
		\begin{equation}
			q_* (s,a) = \sum_{r, s^{\prime}}^{}p(s^{\prime},r|s,a)[r+\gamma v_*(s^{\prime})] 
		\end{equation}
\end{itemize}

\subsection*{Summary}
For all methods the idea is to apply the right hand side RHS of the corresponding Bellman equation repeatedly until it convergence. All methods can also be applied to q(s,a) 

\begin{table}[ht!]
\centering
\begin{tabular}{lll}
 \textbf{Problem}& \textbf{Based on}  &\textbf{Algorithm}  \\ \hline
 Prediction& Bellman equation for $v_\pi$  &Iterative policy evaluation  \\
 Control& Bellman equation for $v_\pi$ + Greedy policy improvement  & Policy iteration \\ 
 Control& Bellman optimality equation& Value iteration \\ \hline
\end{tabular}
\caption{example}
\label{tab:tab1}
\end{table}



