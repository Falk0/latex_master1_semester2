\section{Model-based Reinforcement learning}
This is a class of reinforcement learning algorithms that builds a model of the environments dynamics and reward structure to improve decision making, a policy. By approximating a model of the environment the model-based RL can plan better and faster. The model-based RL uses the model to simulate possible outcomes and optimize the actions accordingly. This approach can lead to more efficient learning, better exploration and improved generalization. Especially in environments with sparse rewards or limited data. 

\subsection*{Repetition, if we do not have a model?}
We can use experience to estimate value functions: $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$ to make new estimates:

	\begin{equation}
		\text{New estimate} \leftarrow \text{Old estimate} + \text{Step size} [\text{Target} - \text{Old estimate}]
	\end{equation}

\begin{wbox}{Repetition}
\begin{itemize}
	\item \textbf{TD-Learning} to estimate $v_\pi(s,a)$
		\begin{equation}
			V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
		\end{equation}
	\item \textbf{SARSA} (on-policy): 
		\begin{equation}
			Q(S_t, A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1} + \gamma Q(S_t,A_t) - Q(S_t,A_t)] 
		\end{equation}
	\item \textbf{Q-learning} (off-policy):
		\begin{equation}
			Q(S_t, A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma \underset{a}{\text{max }}Q(S_t,A_t)]
		\end{equation}
		Possible alternative: Use the experience to estimate a model of the environment. 
\end{itemize}
\end{wbox}


\subsection*{What is a model}
There are many ways to descrive what a model is, but here are two interesting quotes on the subject:


\begin{wbox}{}
\begin{center}
Anything we can use to answer questions about the environment without interacting
with it.” \\– \textbf{Lennart Ljung and Torkel Glad}

\vspace{0.5 cm}

“Essentially, all models are wrong, but some are useful.” \\– \textbf{George E. P. Box}
\end{center}
\end{wbox}

In reinforcement learning a \textbf{model} is something the \textbf{agent} can use to predict how the environment will respond to a given action. It can:
	
\begin{itemize}
	\item Produce a prediction of next state and reward: $(s,a) \rightarrow (\hat{s}^{\prime}, \hat{r}^{\prime}$
	\item Produce a prediction of next state $(s,a) \rightarrow \hat{s}^{\prime}$
\end{itemize}

A \textbf{distribution model} is a model that provides probabilities for all possible possibilities: $p(s^{\prime}, r|s,a)$. This is in general more difficult than the other type of model. \textbf{Sample model} provides us with one random sample from the possible outcomes. 

\subsection*{Why model-based?}




















