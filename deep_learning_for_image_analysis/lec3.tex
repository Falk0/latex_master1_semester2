%!TEX root = master.tex

\section{Feed forward neural networks: Backpropagation}
This lecture will cover how backpropagation works, how to compute the derivatives and some implementation. 

\subsection*{Stepsize}
Stepsize also know as the learning rate controls how large steps we take and greatly affects the training of our model. Using a to small stepsize will cause the convergence to be very slow, it can also be that we get stuck in a local minimum. Using a to big learning rate/stepsize will cause overshooting, bouncing around the minumum but never get there. Or in the worse case it can cause divergence. 



\subsection*{Neural network - construction}
An artificial neural network is a sequential construction of several generalized linear regression models. It consists of \textbf{Inputs} (1, $x_1, x_2, \ldots, x_p$), \textbf{Hidden units} (1, $q_1,q_2,\ldots, q_U$) and an output $\hat{y}$

	\begin{equation}
	\begin{aligned}
		q_1 = h(b^{(1)}_1 + \sum_{j=1}^{p}W^{(1)}_{1j}x_j) \\
		q_2 = h(b^{(1)}_2 + \sum_{j=1}^{p}W^{(1)}_{2j}x_j) \\
		\vdots \quad \quad \quad \quad \vdots \\
		q_U = h(b^{(1)}_U + \sum_{j=1}^{p}W^{(1)}_{Uj}x_j)
	\end{aligned}
	\end{equation}

	\begin{equation}
		\hat{y} = b^{(2)} + \sum_{i=1}^{U}W^{(2)}_iq_i
	\end{equation}


In vector representation:

	\begin{equation}
	\begin{aligned}
		W^{1} = \begin{bmatrix} W^{(1)}_{11} & \ldots& W^{(1)}_{1p} \\ \vdots& & \vdots \\ W^{(1)}_{U1} & \ldots& W^{(1)}_{Up}  \end{bmatrix} , b^{(1)} = \begin{bmatrix} b^{(1)}_1 \\ \vdots \\ b^{(1)}_U \end{bmatrix}, q = \begin{bmatrix} q_1 \\ \vdots \\ q_U \end{bmatrix} \\
		 b^{(2)} = \begin{bmatrix} b^{2} \end{bmatrix}, W^{(2)} = \begin{bmatrix} W^{(2)}_1 \ldots W^{(2)}_U \end{bmatrix} \\
		 \hat{y} = W^{(2)}q + b^{2} ,
	\end{aligned}
	\end{equation}

\subsection*{Deep neural network}
A deep network with several layers will learn better but needs more training. A deep neural network with $L$ can be written like this: 

	\begin{equation}
	\begin{aligned}
		q^{(0)} = \textbf{x} \\
		q^{\ell} = h(\textbf{W}^{(\ell)}q^{(\ell -1)}+\textbf{b}^{(\ell)}), \quad \ell = 1, \ldots, L-1 \\
		\hat{y} = \textbf{W}^{(L)}q^{(L-1)}+ \textbf{b}^{(L)}
	\end{aligned}
	\end{equation}

All the weight matrices and offset vectors in all of the layers are the parameters of the network:

	\begin{equation}
		\textbf{$\theta$} = \{\textbf{W}^{(1)},\textbf{b}^{(1)},\ldots, \textbf{W}^{(L)},\textbf{b}^{(L)}\}
	\end{equation}

These parameters constitutes the parametric model $\hat{y} = f(\textbf{x};\textbf{$\theta$})$ If the number L is large we call this a \emph{deep neural network}.

\subsection*{Unconstrained numerical optimization}
When training the neural network we are considering the following optimization problems: 

	\begin{equation}
		\hat{\textbf{$\theta$}} = \underset{\theta}{\arg \text{min }}J(\textbf{$\theta$}), \quad J(\textbf{$\theta$}) = \frac{1} {n}\sum_{i=1}^{n}L(\textbf{x}_i, \textbf{y}_i, \textbf{$\theta$}) 
	\end{equation}

The \emph{global objective function} or "cost" J is the average loss over the training set. The best solution that can be found will be $\hat{\textbf{$\theta$}}$ which is the global minimizer $(\hat{\textbf{$\theta$}}^{A})$. This global minimizer is often really hard to find and we therefore have to settle for some of the local minimizers $(\hat{\textbf{$\theta$}}^{A},\hat{\textbf{$\theta$}}^{B},\hat{\textbf{$\theta$}}^{C})$. 

\begin{example}{Example: Iterative solution (gradient descent)}
\begin{enumerate}
	\item Pick a $\theta_0$
	\item While (not converged)
		\begin{itemize}
			\item Update $\theta_{t+1} = \theta_t - \gamma \textbf{g}_t$
			\item Update t := t+1 
		\end{itemize}
\end{enumerate}
$\gamma \in \mathbb{R}^{+}$ is the step length or the learning rate.
\end{example}	


\subsection*{How to compute derivatives - Backpropagation}
For every step during the optimization we need to calculate the gradient:

	\begin{equation}
		\textbf{g}_t = \nabla_\theta J{\theta} = \frac{1} {n} \sum_{i=1}^{n}\nabla_\theta L_i(\textbf{x}_i,\textbf{y}_i, \theta)
	\end{equation}

But how do we compute these partial derivative? $\nabla_\theta L_j = (\frac{\partial L_j} {\partial w_1},\frac{\partial L_j} {\partial w_2}, \ldots )$

We using the chain rules and the fact that derivatives propagating backwards up through the net: $\frac{\partial L} {\partial \text{input}} = \frac{\partial L} {\partial \text{output}}\frac{\partial \text{output}} {\partial \text{input}}$. Using a computational graph often helps simplify the understanding and work.

\subsection*{Compute partial derivatives}
We need to calculate the gradient, which is a vector of partial derivatives of the Loss function with respect to the weights $\nabla_\theta L_j = (\frac{\partial L_j} {\partial w_1},\frac{\partial L_j} {\partial w_2}, \ldots )$. Here the Loss is given by a rather nasty looking expression involving weight, images and outputs:

	\begin{equation}
		L(\text{Net}(x_i,\theta) = L(s(W_3h(W_2h(W_1)))),y_i) 
	\end{equation}

Which is for a given set of images: $\{x_i\}$ and the correct output $\{y_i\}$, the variables are all our model parameters: weight and offsets. If we assuming in total m number of tunable parameters, then this is a function:

	\begin{equation}
		f : R^{m} \rightarrow R
	\end{equation}

The partial derivative of f at a point \textbf{a} = $(a_1, \ldots, a_j, \ldots,a_m)$ with respect to the j-th variable $x_j$ is the following expression:

	\begin{equation}
		\frac{\partial} {\partial x_j}f(\textbf{a}) =  \lim_{h \rightarrow 0} \frac{f(a_1,\ldots, a_j +h, \ldots, a_m) - f(a_1, \ldots, a_j, \ldots, a_m)} {h} 
	\end{equation}

This tells us how the function f change, as we move a little bit, the length $h$, along the dimension $j$. There are two ways to do this:

\begin{itemize}
	\item The \textbf{numerical gradient}: which is an approximation, slow to compute but easy to write.
	\item The \textbf{analytic gradient}: which is fast to compute and exact but quite error prone.
\end{itemize}

In practice we can derive the analytic gradient, check the implementation with a numerical gradient to see if it works. To derive the analytic gradients it is simply putting the chain rule to use. Since our function f is a composition of other functions f(g(h(...))) we compose the problem into smaller simpler problems:

	\begin{equation}
	\begin{aligned}
		f(x,y,z) = (x+y) * z \\
		f(q,z) = q * z \\
		q(x,y) = x + y \\
		q(x,y) = x + y \\
		\frac{\partial f} {\partial x} = \frac{\partial f} {\partial q} \frac{\partial q} {\partial x} 
	\end{aligned}
	\end{equation}

The gradient (propagating backwards on the input side) is the local gradient of the node multiplied multiplied with the gradient arriving to the node from the output side. If the node is connected to multiple output nodes, sum up the gradients of the different paths. When using ReLu activation only pass gradients to the positive outputs. With deep neural networks there are many values multiplied together and this can lead to problems with vanishing or exploding gradients. A ReLu that never turns on during the training, will never move (zero gradient) these are called "a dead ReLu".


\subsection*{Summary}

\begin{itemize}
	\item \textbf{Neural network (NN)}: A nonlinear \emph{parametric} model constructed by stacking several linear models with intermediate nonlinear activation functions.
	\item \textbf{Activation function}: A nonlinear scalar function applied to each output element of the linear models in a NN.
	\item \textbf{Gradient descent}: An iterative optimization algorithm where we at each iteration take a step proportional to the negative gradient.
	\item \textbf{Learning rate}: A scalar and tunable parameter which decide the length of each gradient step in gradient descent.
	\item \textbf{Back-propagation}: An efficient method that is based on the chain rule to compute the gradients.  
\end{itemize}






